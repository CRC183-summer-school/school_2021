{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6623b1e1",
   "metadata": {},
   "source": [
    "### CRC183 Summer School \"Machine Learning in Condensed Matter Physics\"\n",
    "# Hands-on session: Building deep learning models with JAX/Flax\n",
    "\n",
    "## Installing JAX, NetKet & Co.\n",
    "\n",
    "In today's session you will learn about basics of JAX and the Netket library. Installing NetKet is relatively straightforward and it will automatically install JAX as a dependency. For this Tutorial, if you are running it locally on your machine, we recommend that you create a clean virtual environment and install NetKet within: \n",
    "\n",
    "```bash\n",
    "python3 -m venv netket\n",
    "source netket/bin/activate\n",
    "pip install --pre netket\n",
    "```\n",
    "\n",
    "If you are wondering why we use the flag ```--pre``` it is because today we will be working on a pre (beta) release of version 3.0. \n",
    "\n",
    "**If you are on Google Colab**, run the following cell to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre -U netket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2d586",
   "metadata": {},
   "source": [
    "You can check that the installation was succesfull doing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7bc09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9661c",
   "metadata": {},
   "source": [
    "## JAX\n",
    "\n",
    "[JAX](https://github.com/google/jax) is a Python library that provides essential functionality for deep learning applications, namely\n",
    "\n",
    "* **automatic differentiation**\n",
    "* **vectorization**\n",
    "* **just-in-time compilation** to GPU/TPU accelerators\n",
    "\n",
    "It is being developed \"for high-performance machine learning research\" and as such well suited for physics applications. The abovementioned features are implemented in a system of composable function transformations that \"process\" Python functions.\n",
    "\n",
    "## `jax.numpy`\n",
    "\n",
    "The function transformations of JAX rely on functions being written in a \"JAX-intellegible\" form. For this purpose the `jax.numpy` sub-module emulates almost the whole NumPy functionality. When using JAX, any array operations should be written using `jax.numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fdb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3469dcf2",
   "metadata": {},
   "source": [
    "The basic object is the `jnp.array`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e9405",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = jnp.array([1,2,3])\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e521b",
   "metadata": {},
   "source": [
    "NumPy arrays and `jax.numpy` arrays can be converted into each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(arr)\n",
    "type(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(jnp.array(np_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7d4ae",
   "metadata": {},
   "source": [
    "Most NumPy functions have an equivalent in `jax.numpy`, e.g. `sum()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnp.sum(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d715676f",
   "metadata": {},
   "source": [
    "## JAX random numbers\n",
    "\n",
    "JAX implements a pseudo-random number generator (PRNG) in the `jax.random` submodule. The JAX PRNG handles the state of the PRNG explicitly in the form of *keys* that have to be passed around, and which can be split in order to fork the PRNG state into new PRNGs.\n",
    "\n",
    "Let's generate an initial key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc7a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1234\n",
    "key = jax.random.PRNGKey(seed)\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119fcba0",
   "metadata": {},
   "source": [
    "Any function in `jax.random` that generates random numbers takes a key as argument, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c171905",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.random.normal(key, (3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3bd8d2",
   "metadata": {},
   "source": [
    "Passing the same key results in the same set of random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.random.normal(key, (3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d760d559",
   "metadata": {},
   "source": [
    "To get a different sequence of random numbers, we need to generate a new key by splitting the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebbcbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, newkey = jax.random.split(key)\n",
    "\n",
    "jax.random.normal(newkey, (3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef25ab",
   "metadata": {},
   "source": [
    "## Warning of \"sharp edges\" in JAX\n",
    "\n",
    "The ability to perform the various function transformations imposes a number of constraints on how those functions are written. Before diving deeper into the JAX library, reading [\"JAX - the sharp bits\"](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html), which summarizes the main pitfalls when working with it, is highly recommended.\n",
    "\n",
    "The most important points are\n",
    "\n",
    " * Only pure functions\n",
    " * Control flow needs some care\n",
    " * JAX arrays are immutable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f56aa0",
   "metadata": {},
   "source": [
    "## Flax\n",
    "\n",
    "[Flax](https://github.com/google/flax) builds on JAX and provides a framework to easily define arbitrary deep learning models very similar to PyTorch. Moreover, typical building blocks of deep learning models are already implemented in the library.\n",
    "\n",
    "As an example, let's define a simple feed-forward network with a single layer. It consists of a dense layer, i.e., an affine-linear transformation of the input followed by a $\\text{ReLU}(\\cdot)$ non-linearity giving the *activations*. Additionally, we include a reduction operation to obtain a single number as output, namely the sum of the activations:\n",
    "\n",
    "$$\n",
    "f_{W,b}(x) = \\sum_i \\text{ReLU}(W_{i,j}x_j+b_i)\n",
    "$$\n",
    "\n",
    "In Flax this neural network can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47107e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "class MyFFN(nn.Module):\n",
    "    num_neurons: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        z = nn.Dense(self.num_neurons)(x) # affine-linear transformation of the input\n",
    "        a = nn.relu(z) # non-linearity\n",
    "        \n",
    "        return jnp.sum(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29814073",
   "metadata": {},
   "source": [
    "The code above shows the basic syntax to define a custom model in Flax. A Flax model is a class that inherits from the abstract `flax.linen.Module` class. As part of it we have to implement a `__call__` method that defines how the network is evaluated for a given input.\n",
    "\n",
    "In the example the `__call__` method is decorated with `@nn.compact`, because we are using the compact definition of a Flax model. This is possible for simple models where we do not have to specify an additional setup procedure for the initialization of the model. For more complex models a `setup` method can be defined for the initialization; in that case the `@nn.compact` decorator has to be dropped.\n",
    "\n",
    "In addition, we defined the data field `num_neurons`. The data fields of the model class are initialized by the constructor and they can contain the hyperparameters of the model.\n",
    "\n",
    "Finally, we use some of the pre-implemented building blocks, namely the `nn.Dense` layer for the affine-linear transformation and the activation function `nn.relu`.\n",
    "\n",
    "Through the `nn.Module` base class our `MyFFN` class will have further methods that allow us to deal with the model. Below we will learn about `init` for parameter intialization and `apply` for the evaluation.\n",
    "\n",
    "Now we can create an instance of this class, passing a value for the `num_neurons` hyperparameter to the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyFFN(num_neurons=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307c4fa",
   "metadata": {},
   "source": [
    "Before we can work with this model, we also need to get an initial set of parameters. For this purpose we have to call the model's `init` method, which takes an `jax.random.PRNGKey` and an exemplary input datum as arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = jax.random.normal(jax.random.PRNGKey(4321), (28*28,))\n",
    "params = net.init(jax.random.PRNGKey(1234), example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a1895",
   "metadata": {},
   "source": [
    "The set of parameters is returned in the form of a tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44faec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251ce580",
   "metadata": {},
   "source": [
    "Now we can evaluate our neural network on the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53318b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.apply(params, example_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
