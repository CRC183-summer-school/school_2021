{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a057a-8f2b-49f8-829b-3df2f9acb3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell, if you are running the notebook on Google Colab:\n",
    "!git clone https://github.com/CRC183-summer-school/school_2021.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e4be9ead-0f43-4a13-a6e1-6d7fdd4dc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Everything we need from JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9632d9-3e89-452e-b071-f9f1913b0224",
   "metadata": {},
   "source": [
    "## Activation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5b13e3a-5c43-4fac-bdc0-b1d5c874138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+jnp.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a3158-7210-4898-872d-f9188aa1f756",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "848b8b97-c284-48f8-9216-5496c9a9e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key for our random number generator, we need this because JAX will distribute the calculation\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "def initialize_network_params(layer_sizes):\n",
    "    # Get a random key for each layer\n",
    "    keys = random.split(key, len(layer_sizes))\n",
    "    \n",
    "    parameters = []\n",
    "    for i in range(len(layer_sizes)-1):\n",
    "        parameters.append( [random.normal(keys[i], (layer_sizes[i+1], layer_sizes[i])), # The weight matrix\n",
    "                            random.normal(keys[i], (layer_sizes[i+1],)) ]) # The bias\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "61cbedde-7959-486f-9392-0a8dac1c95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [50, 10, 2]\n",
    "neural_network_parameters = initialize_network_params(layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a18d475f-13b7-42af-b843-92d60b636373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(params, sample):\n",
    "    activations = sample\n",
    "    \n",
    "    # Feed through the network, layer by layer\n",
    "    for weights, biases in params[:-1]:\n",
    "        outputs = jnp.dot(weights, activations) + biases\n",
    "        activations = sigmoid(outputs)\n",
    "\n",
    "    # For the final layer, we don't want the relu\n",
    "    final_weight, final_bias = params[-1]\n",
    "    logits = jnp.dot(final_weight, activations) + final_bias\n",
    "    \n",
    "    # We want a softmax output\n",
    "    explogits = jnp.exp(logits)\n",
    "    return explogits/jnp.sum(explogits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7b4cb472-c494-4b2e-bd02-46619f46e6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.11788198, 0.88211805], dtype=float32)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sample(neural_network_parameters, np.zeros(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f037979-9e91-4b96-bdeb-2d4fe94f2de4",
   "metadata": {},
   "source": [
    "## The Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "744a3f0a-b318-4f3c-bad3-4d7505f78648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_for_sample(params, sample, target):\n",
    "    pred = predict_sample(params, sample)\n",
    "    return jnp.mean( (pred - target)**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "42945a0e-1472-4158-aa39-65b2d7c8bbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.7781322, dtype=float32)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_for_sample(neural_network_parameters, np.zeros(50), np.array([1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9e034-e03d-484e-b396-7bb5eb5a3b55",
   "metadata": {},
   "source": [
    "## The optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "7d83ce75-20f3-4b98-bc97-b5b9da86ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1e-2\n",
    "def update_for_sample(params, x, y):\n",
    "    grads = grad(loss_for_sample)(params, x, y)\n",
    "    return [(w - step_size * dw, b - step_size *db) for (w,b),(dw,db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "57d662f7-810c-47e8-b133-787b01a1b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_parameters = update_for_sample(neural_network_parameters, np.zeros(50), np.array([1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cccfb60-17f2-4412-939c-4e8170360e48",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f0092716-fb0d-4119-9c03-b111f5827da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prediction: \n",
      "[0.49242866 0.5075713 ]\n",
      "After training: \n",
      "[0.08945558 0.9105444 ]\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [50, 10, 2]\n",
    "neural_network_parameters = initialize_network_params(layer_sizes)\n",
    "\n",
    "print(\"Initial prediction: \")\n",
    "print(predict_sample(neural_network_parameters, np.zeros(50)))\n",
    "\n",
    "for epoch in range(500):\n",
    "    # Update the neural network parameters, so that for input 'np.zeros(50)' the output becomes '[0,1]'\n",
    "    neural_network_parameters = update_for_sample(neural_network_parameters, np.zeros(50), np.array([0,1]))\n",
    "    \n",
    "print(\"After training: \")\n",
    "print(predict_sample(neural_network_parameters, np.zeros(50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510dd22-0180-4e61-81cc-813d7984f9c4",
   "metadata": {},
   "source": [
    "### If we now want to predict for many samples, we **could** do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "221df96c-ee18-4366-80e1-6317537a6c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.9115486\n"
     ]
    }
   ],
   "source": [
    "# Generate 10 random samples of size 50\n",
    "all_samples = np.random.rand(10,50)\n",
    "# For demonstration purposes, let's have the same target for all samples\n",
    "target = np.array([1,0])\n",
    "\n",
    "total_loss = 0\n",
    "for sample in all_samples:\n",
    "    total_loss += loss_for_sample(neural_network_parameters, sample, target)\n",
    "    \n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473fe20-94a9-4723-8f7d-9a9290aeea5c",
   "metadata": {},
   "source": [
    "#### But JAX can do this much faster! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "3599190f-bbf3-4ec9-9762-823ff1606922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 'vectorized' version of predict_sample, using 'vmap'\n",
    "# Here we tell JAX that the index to be vectorized over is the first (in_axis = 0), and that it \n",
    "#   should also put that vectorized result as the first index in the output\n",
    "predict = vmap(predict_sample, in_axes=(None,0), out_axes=0)\n",
    "\n",
    "# Now we (re-)define the loss function to use this new vectorized predict function\n",
    "def loss(params, samples, targets):\n",
    "    pred = predict(params, samples)\n",
    "    return jnp.mean( (pred - targets)**2 )\n",
    "\n",
    "# And the same for the update function\n",
    "step_size = 1e-2\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return [(w - step_size * dw, b - step_size *db) for (w,b),(dw,db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6e8cdef2-3bb2-423d-9d30-df215d5ff398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prediction: \n",
      "[[0.26412225 0.73587775]\n",
      " [0.24111044 0.75888956]\n",
      " [0.13203025 0.86796975]\n",
      " [0.38874215 0.61125785]\n",
      " [0.396364   0.60363597]\n",
      " [0.18825619 0.81174374]\n",
      " [0.39966217 0.6003378 ]\n",
      " [0.07824132 0.9217587 ]\n",
      " [0.2613949  0.7386051 ]\n",
      " [0.26985535 0.7301446 ]]\n",
      "After training: \n",
      "[[0.07209282 0.92790717]\n",
      " [0.05102606 0.9489739 ]\n",
      " [0.04807254 0.9519275 ]\n",
      " [0.08966338 0.9103367 ]\n",
      " [0.10765581 0.89234424]\n",
      " [0.05388276 0.9461173 ]\n",
      " [0.11235292 0.8876471 ]\n",
      " [0.01764767 0.9823523 ]\n",
      " [0.06969136 0.93030864]\n",
      " [0.06187989 0.9381201 ]]\n"
     ]
    }
   ],
   "source": [
    "all_samples = np.random.rand(10,50)\n",
    "targets = np.array([[0,1] for i in range(10)])\n",
    "\n",
    "layer_sizes = [50, 10, 2]\n",
    "neural_network_parameters = initialize_network_params(layer_sizes)\n",
    "\n",
    "print(\"Initial prediction: \")\n",
    "print(predict(neural_network_parameters, all_samples))\n",
    "\n",
    "for epoch in range(500):\n",
    "    neural_network_parameters = update(neural_network_parameters, all_samples, targets)\n",
    "    \n",
    "print(\"After training: \")\n",
    "print(predict(neural_network_parameters, all_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b66a3e-0b9a-4761-bb9a-283c2e442c8e",
   "metadata": {},
   "source": [
    "# Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d124f6c-b75d-49b6-a758-d8fb85be7cea",
   "metadata": {},
   "source": [
    "Instead of defining the model parameters for this network ourselves, the module `Flax` is very useful for defining neural networks. Especially once you would like to build more complex ones (convolutional layers, for example), it is a useful module to have on top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2a9fd-c9e6-4aaa-9eb7-057e46cb86d3",
   "metadata": {},
   "source": [
    "Additionally, `Flax` has the `flax.optim` module that conveniently has all kinds of optimizers. As an alternative, `jax.experimental` has a bunch of optimizers too!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
