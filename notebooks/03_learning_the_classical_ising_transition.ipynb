{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e7b694",
   "metadata": {},
   "source": [
    "Run this cell, if you are running the notebook on Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f116e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/CRC183-summer-school/school_2021.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb95c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# To measure the time it takes to train a network\n",
    "import time\n",
    "\n",
    "# Everything we need from JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "from jax.scipy.special import logsumexp # Computes the log of the sum of exponentials of input elements.\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "# Flax module for deep learning models\n",
    "import flax.linen as nn\n",
    "\n",
    "import sys\n",
    "sys.path.append(sys.path[0] + \"./school_2021/notebooks/\")\n",
    "\n",
    "from IsingData import generate_Ising_configurations\n",
    "\n",
    "# The key for our random number generator, we need this because JAX will distribute the calculation\n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c5b564",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306768e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The temperatures that we are going to generate samples at\n",
    "Ts = np.arange(1.95, 0.04, -0.1) * 2.27\n",
    "\n",
    "# For a few different system sizes, store the data in a dictionary with L as key\n",
    "all_data = generate_Ising_configurations(10, 1000, Ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bae067-3b9e-4056-a799-42dfa27e163f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a177ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(all_data, Ts, Tc=2.7, train_fraction=0.8):\n",
    "    # Lists to store the raw data\n",
    "    raw_T = []\n",
    "    raw_x = []\n",
    "    raw_y = [] \n",
    "    \n",
    "    for T in Ts:      \n",
    "        raw_x.append(all_data['%.3f'%(T)])\n",
    "        n = len(all_data['%.3f'%(T)])\n",
    "        label = [1,0] if T < Tc else [0,1]\n",
    "        raw_y.append(np.array([label] * n))\n",
    "        raw_T.append(np.array([T]*n))\n",
    "        \n",
    "    raw_T = np.concatenate(raw_T)\n",
    "    raw_x = np.concatenate(raw_x, axis=0)\n",
    "    raw_y = np.concatenate(raw_y, axis=0)\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(len(raw_x))\n",
    "    all_T = raw_T[indices]\n",
    "    all_x = raw_x[indices]\n",
    "    all_y = raw_y[indices]\n",
    "\n",
    "    # Split into train and test sets\n",
    "    train_split = int(train_fraction * len(all_x))\n",
    "    train_T = jnp.array(all_T[:train_split])\n",
    "    train_x = jnp.array(all_x[:train_split])\n",
    "    train_y = jnp.array(all_y[:train_split])\n",
    "    test_T = jnp.array(all_T[train_split:])\n",
    "    test_x = jnp.array(all_x[train_split:])\n",
    "    test_y = jnp.array(all_y[train_split:])\n",
    "    \n",
    "    return [raw_T, raw_x, raw_y], [train_T, train_x, train_y], [test_T, test_x, test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f4408",
   "metadata": {},
   "source": [
    "### Visualize some data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5908f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[raw_T, raw_x, raw_y], [train_T, train_x, train_y], [test_T, test_x, test_y] = get_training_data(all_data,Ts)\n",
    "\n",
    "fig, ax = plt.subplots(4,5, figsize=(14,10))\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        ax[i,j].matshow(np.reshape(raw_x[(4*i + j)*800], (10,10)), cmap='Greys')\n",
    "        ax[i,j].set_title(\"$T = %.3f$\"%raw_T[(4*i + j)*800])\n",
    "        ax[i,j].set_xticks([])\n",
    "        ax[i,j].set_yticks([])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f2699",
   "metadata": {},
   "source": [
    "## Make a Feedforward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6682892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    layers: Sequence[int] # A tuple that contains the widths of all layers follwing the input layer\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "\n",
    "        a = x.ravel() # flatten the input\n",
    "\n",
    "        # Evaluate network layer by layer\n",
    "        for width in self.layers[:-1]:\n",
    "            # Apply a the Dense layer with given width followed by the non-linearity\n",
    "            a = nn.relu(nn.Dense(width)(a))\n",
    "          \n",
    "        a = nn.Dense(self.layers[-1])(a)\n",
    "        \n",
    "        # Return activations of the output layer\n",
    "        return a - logsumexp(a)\n",
    "    \n",
    "# Initialize a new network (or rather its parameters)\n",
    "layer_sizes = [100, 64, 32, 2]\n",
    "net = MyNet(layers=layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7872cee8",
   "metadata": {},
   "source": [
    "### Define the forward prediction of our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b965a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a batched `predict` function\n",
    "predict = jax.vmap(lambda p, x: net.apply(p,x), in_axes=(None, 0), out_axes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930028bb",
   "metadata": {},
   "source": [
    "## The loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928be7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, images, targets):\n",
    "    preds = predict(params, images)\n",
    "    return -jnp.mean(preds * targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd33e6a",
   "metadata": {},
   "source": [
    "## The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7633aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an optimizer in Jax\n",
    "step_size = 1e-3\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y, opt_state):\n",
    "    \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "    value, grads = value_and_grad(loss)(params, x, y)\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return get_params(opt_state), opt_state, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226be263",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b3994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(params, images, targets):\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(predict(params, images), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "def train(train_x, train_y, test_x, test_y, num_epochs, batch_size, opt_state):\n",
    "    \"\"\" Implements a learning loop over epochs. \"\"\"\n",
    "    # Initialize placeholder for loggin\n",
    "    log_acc_train, log_acc_test, train_loss = [], [], []\n",
    "\n",
    "    # Get the initial set of parameters\n",
    "    params = get_params(opt_state)\n",
    "\n",
    "    # Get initial accuracy after random init\n",
    "    train_acc = accuracy(params, train_x, train_y)\n",
    "    test_acc = accuracy(params, test_x, test_y)\n",
    "    log_acc_train.append(train_acc)\n",
    "    log_acc_test.append(test_acc)\n",
    "\n",
    "    # Divide into batches\n",
    "    num_batches = len(train_x) // batch_size\n",
    "\n",
    "    # Loop over the training epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(len(train_x))\n",
    "        batch_indices = jnp.split(indices[:num_batches*batch_size], batch_size)\n",
    "        \n",
    "        for b in range(len(batch_indices)):\n",
    "            x = train_x[batch_indices[b]]\n",
    "            y = train_y[batch_indices[b]]\n",
    "            \n",
    "            params, opt_state, loss = update(params, x, y, opt_state)\n",
    "            train_loss.append(loss)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        train_acc = accuracy(params, train_x, train_y)\n",
    "        test_acc = accuracy(params, test_x, test_y)\n",
    "        log_acc_train.append(train_acc)\n",
    "        log_acc_test.append(test_acc)\n",
    "        print(\"Epoch {} | Time: {:0.2f} | Train A: {:0.3f} | Test A: {:0.3f}\".format(epoch+1, epoch_time,\n",
    "                                                                    train_acc, test_acc))\n",
    "\n",
    "        \n",
    "    return train_loss, log_acc_train, log_acc_test, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13119c0f",
   "metadata": {},
   "source": [
    "# Blanking: train model in known limits, then use it to predict the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87cf933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick only the Ts at the beginning and at the end, and construct a dataset\n",
    "train_Ts = list(Ts[:4]) + list(Ts[-4:])\n",
    "[raw_T, raw_x, raw_y], [train_T, train_x, train_y], [test_T, test_x, test_y] = get_training_data(all_data, train_Ts)\n",
    "\n",
    "# Initialize a new network (or rather its parameters)\n",
    "params = MyNet(layers=layer_sizes).init(key, train_x[0])\n",
    "\n",
    "# Initialize the optimizer\n",
    "opt_state = opt_init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd810aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_log, test_log, params = train(train_x, train_y, test_x, test_y, 20, 32, opt_state)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=200)\n",
    "ax.plot(train_loss)\n",
    "ax.set_xlabel(\"Batch\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c0daf",
   "metadata": {},
   "source": [
    "## Predict for all Ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafcbeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every temperature, get the averaged prediction\n",
    "predictions = []\n",
    "for T in Ts:\n",
    "    x = all_data['%.3f'%T]#['x']\n",
    "    p = jnp.exp( predict(params, x) )  # Exponentiate it to go back to the [0,1] range for nicer plotting\n",
    "    p = jnp.mean( p, axis=0 )\n",
    "    predictions.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "ax.plot( Ts, predictions )\n",
    "ax.set_xlabel(\"T\")\n",
    "ax.set_ylabel(\"Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff40f8",
   "metadata": {},
   "source": [
    "# Challenge ideas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a sigmoid to the outputs to extract the crossing point (=Tc)\n",
    "# See how the crossing point depends on:\n",
    "    # The (width of the) regions in which you train\n",
    "    # The network architecture\n",
    "    # Size of the data (instead of 10x10 snapshots)\n",
    "    \n",
    "# Convert to use a Flax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1fd049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81311271-7835-4d17-af71-b28c917e56a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c94cfb6d-56bb-428d-8780-4929c69ff617",
   "metadata": {},
   "source": [
    "# Learning by confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d29526-c773-4567-8ff6-5c94d6f1665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_guess(Ts, Tc_guess):\n",
    "    [raw_T, raw_x, raw_y], [train_T, train_x, train_y], [test_T, test_x, test_y] = get_training_data(all_data, Ts, Tc=Tc_guess, train_fraction=0.9)\n",
    "    \n",
    "    # Initialize a new network (or rather its parameters)\n",
    "    layer_sizes = [100, 32, 16, 2]\n",
    "    params = initialize_network_params(layer_sizes, key)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    opt_state = opt_init(params)\n",
    "    \n",
    "    train_loss, train_log, test_log, params = train(train_x, train_y, test_x, test_y, 30, 16, opt_state)#, verbose=False)\n",
    "    \n",
    "    return train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a91ad-bdbd-469b-ba5a-edcf5cdaa445",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "W_over_time = {}\n",
    "for guess in np.linspace(Ts[0], Ts[-1], 10):\n",
    "    print(\"Training at guess %.3f\"%guess)\n",
    "    accuracy_vs_epoch = train_with_guess(Ts, guess)\n",
    "    W_over_time[guess] = accuracy_vs_epoch\n",
    "    print(\"\\t Final accuracy: %.3f\"%accuracy_vs_epoch[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd82fbf-445c-4582-916e-56aa81e8808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_Ts = np.linspace(Ts[0], Ts[-1], 10)\n",
    "\n",
    "for i in range(1,30):\n",
    "    curve = [W_over_time[guess][i] for guess in guess_Ts]\n",
    "    color = np.array([0.7, 0.7, 0.7])*i/30\n",
    "    plt.plot( guess_Ts, curve, '-o', c=color )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d0b27-64cb-4147-98c8-26b756f02953",
   "metadata": {},
   "source": [
    "## Challenge ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945de10-1316-47c0-bc4a-8c78789077c7",
   "metadata": {},
   "source": [
    "* Play with the hyperparameters to build intuition, see if you can get a clear W shape\n",
    "* Increase the temperature resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be5978d-e512-4344-a4db-a278c8fd58f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8dda84b-ff25-4da8-a236-5d5d0596eedc",
   "metadata": {},
   "source": [
    "# Discriminative Cooperative Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5065cd-a205-44c3-af16-04092cc72325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement https://arxiv.org/abs/1706.08111! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb227ad-31da-402d-b362-68fe60506751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_seminar]",
   "language": "python",
   "name": "conda-env-ml_seminar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
